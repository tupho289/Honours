{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 0. PREPARATION ############################\n",
    "\n",
    "#-------------------------- import packages --------------------------\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Multiply, Add\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.compose import make_column_transformer\n",
    "from NAM_models import ActivationLayer, FeatureNN, NAM\n",
    "\n",
    "#-------------------------- import data --------------------------\n",
    "'''we use the popular French Motor TPL insurance claim data '''\n",
    "freq = pd.read_csv(\"data/freMTPL2freq.csv\")\n",
    "sev = pd.read_csv(\"data/freMTPL2sev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 1. PREPROCESSING ############################\n",
    "\n",
    "random.seed(2000)\n",
    "\n",
    "#-------------------------- merge/filter claim data --------------------------\n",
    "# complete claim severity data\n",
    "claimsev = sev.merge(freq, on = 'IDpol', how = 'left')\n",
    "claimsev = claimsev.drop(columns = ['ClaimNb', 'Exposure'])\n",
    "\n",
    "# drop ID\n",
    "claimfreq = freq.drop(columns = \"IDpol\")\n",
    "\n",
    "\n",
    "#-------------------------- subsample and split --------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    claimfreq.drop(\"ClaimNb\", axis = 1), claimfreq[\"ClaimNb\"], random_state = 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exposure': 1,\n",
       " 'VehPower': 1,\n",
       " 'VehAge': 1,\n",
       " 'DrivAge': 1,\n",
       " 'BonusMalus': 1,\n",
       " 'VehBrand': 11,\n",
       " 'VehGas': 1,\n",
       " 'Area': 1,\n",
       " 'Density': 1,\n",
       " 'Region': 21}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------- feature transformation --------------------------\n",
    "# define transformer\n",
    "ct = make_column_transformer(\n",
    "    (\"passthrough\", [\"Exposure\"]),\n",
    "    (OrdinalEncoder(), [\"Area\", \"VehGas\"]),\n",
    "    (OneHotEncoder(), [\"VehBrand\", \"Region\"]),\n",
    "    remainder = StandardScaler(),\n",
    "    verbose_feature_names_out = False\n",
    ")\n",
    "\n",
    "\n",
    "# fit & transform\n",
    "train = ct.fit_transform(X_train).toarray()\n",
    "test = ct.transform(X_test).toarray()\n",
    "feature_names = ct.get_feature_names_out()  # get the columns' names\n",
    "\n",
    "\n",
    "# number of columns for each feature after transformation\n",
    "feature_expansion = {} # empty dictionary to store the output\n",
    "for original_feature in X_train.columns:\n",
    "    # For each original feature, count how many transformed feature names start with it\n",
    "    count = sum(fn.startswith(original_feature) for fn in feature_names)\n",
    "    feature_expansion[original_feature] = count\n",
    "feature_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 2. NAM ############################\n",
    "\n",
    "#-------------------------- define functions to create NAM and subnets --------------------------\n",
    "'''create subnetwork for each feature/group of features'''\n",
    "def create_subnet(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation = 'relu'),\n",
    "        Dense(32, activation = 'relu'),\n",
    "        Dense(1, activation = \"exponential\")  # output layer for the subnet, assuming a scalar output\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''create NAM'''\n",
    "def create_nam_model(feature_expansion):\n",
    "    inputs = [] # store the input layer of each subnet \n",
    "    sub_outputs = [] # store the output of each subnetwork\n",
    "    for column in feature_expansion:\n",
    "        input_layer = Input(shape = (feature_expansion[column],))\n",
    "        inputs.append(input_layer)\n",
    "        \n",
    "        if column == \"Exposure\":\n",
    "            # assume the first input is the exposure, we directly use it without a subnet\n",
    "            exposure_input = input_layer\n",
    "        else:\n",
    "            # create subnet for each feature/group of features\n",
    "            subnet = create_subnet(feature_expansion[column])\n",
    "            sub_output = subnet(input_layer)\n",
    "            sub_outputs.append(sub_output)\n",
    "    \n",
    "    # concatenate the outputs of the subnets and sum them\n",
    "    sum_of_subs = Add()(sub_outputs)\n",
    "    \n",
    "    # multiply the exposure input by the sum of subnets' outputs\n",
    "    final_output = Multiply()([exposure_input, sum_of_subs])\n",
    "    \n",
    "    # final model\n",
    "    model = Model(inputs = inputs, outputs = final_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2292 - mean_squared_error: 0.1404 - poisson: 0.2292 - val_loss: 0.2102 - val_mean_squared_error: 0.0587 - val_poisson: 0.2102\n",
      "Epoch 2/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2079 - mean_squared_error: 0.0565 - poisson: 0.2079 - val_loss: 0.2081 - val_mean_squared_error: 0.0589 - val_poisson: 0.2081\n",
      "Epoch 3/50\n",
      "12713/12713 [==============================] - 20s 2ms/step - loss: 0.2067 - mean_squared_error: 0.0565 - poisson: 0.2067 - val_loss: 0.2081 - val_mean_squared_error: 0.0586 - val_poisson: 0.2081\n",
      "Epoch 4/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2059 - mean_squared_error: 0.0564 - poisson: 0.2059 - val_loss: 0.2070 - val_mean_squared_error: 0.0591 - val_poisson: 0.2070\n",
      "Epoch 5/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2055 - mean_squared_error: 0.0564 - poisson: 0.2055 - val_loss: 0.2068 - val_mean_squared_error: 0.0587 - val_poisson: 0.2068\n",
      "Epoch 6/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2052 - mean_squared_error: 0.0564 - poisson: 0.2052 - val_loss: 0.2069 - val_mean_squared_error: 0.0585 - val_poisson: 0.2069\n",
      "Epoch 7/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2051 - mean_squared_error: 0.0564 - poisson: 0.2051 - val_loss: 0.2061 - val_mean_squared_error: 0.0587 - val_poisson: 0.2061\n",
      "Epoch 8/50\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2049 - mean_squared_error: 0.0563 - poisson: 0.2049 - val_loss: 0.2062 - val_mean_squared_error: 0.0586 - val_poisson: 0.2062\n",
      "Epoch 9/50\n",
      "12713/12713 [==============================] - 20s 2ms/step - loss: 0.2047 - mean_squared_error: 0.0563 - poisson: 0.2047 - val_loss: 0.2068 - val_mean_squared_error: 0.0592 - val_poisson: 0.2068\n",
      "Epoch 10/50\n",
      "12713/12713 [==============================] - 20s 2ms/step - loss: 0.2046 - mean_squared_error: 0.0563 - poisson: 0.2046 - val_loss: 0.2068 - val_mean_squared_error: 0.0590 - val_poisson: 0.2068\n",
      "Epoch 11/50\n",
      "12713/12713 [==============================] - 20s 2ms/step - loss: 0.2045 - mean_squared_error: 0.0563 - poisson: 0.2045 - val_loss: 0.2074 - val_mean_squared_error: 0.0585 - val_poisson: 0.2074\n",
      "Epoch 12/50\n",
      "12698/12713 [============================>.] - ETA: 0s - loss: 0.2044 - mean_squared_error: 0.0563 - poisson: 0.2044Restoring model weights from the end of the best epoch: 7.\n",
      "12713/12713 [==============================] - 21s 2ms/step - loss: 0.2044 - mean_squared_error: 0.0563 - poisson: 0.2044 - val_loss: 0.2065 - val_mean_squared_error: 0.0586 - val_poisson: 0.2065\n",
      "Epoch 12: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2061309516429901"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------- fit a NAM to training data --------------------------\n",
    "\n",
    "model_NAM = create_nam_model(feature_expansion) # create NAM\n",
    "model_NAM.compile(optimizer = \"adam\", loss = \"poisson\", metrics = ['mean_squared_error', 'poisson'])\n",
    "es = EarlyStopping(patience = 5, restore_best_weights = True, verbose = 1)\n",
    "\n",
    "# training data need to be split into different arrays, each correponds to input for a particular subnet\n",
    "start = 0\n",
    "X_train_split = []\n",
    "for size in feature_expansion.values():\n",
    "    end = start + size\n",
    "    X_train_split.append(train[:, start:end])\n",
    "    start = end\n",
    "\n",
    "# fit the model\n",
    "model_NAM.fit(X_train_split, y_train, epochs = 50, batch_size = 32, callbacks = [es], validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- plot model --------------------------\n",
    "plot_model(model_NAM, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 2s 465us/step - loss: 0.2055 - mean_squared_error: 0.0572 - poisson: 0.2055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20545218884944916, 0.057174332439899445, 0.20545218884944916]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------- evaluate on testing data --------------------------\n",
    "\n",
    "# split testing data into different sets\n",
    "start = 0\n",
    "X_test_split = []\n",
    "for size in feature_expansion.values():\n",
    "    end = start + size\n",
    "    X_test_split.append(test[:, start:end])\n",
    "    start = end\n",
    "\n",
    "# evaluation\n",
    "model_NAM.evaluate(X_test_split, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 3. GLM ############################\n",
    "\n",
    "#-------------------------- preprocessing --------------------------\n",
    "'''we want to use dummy encoding for VehBrand and Region so 2 columns need\n",
    "to be removed from both training and test data. These 2 columns are the reference\n",
    "levels for VehBrand and Region. Choose B12 for VehBrand and Centre for Region.'''\n",
    "\n",
    "# separate the offset term (or exposure)\n",
    "offset_train = train[:, 0]\n",
    "offset_test = test[:, 0]\n",
    "\n",
    "# index for reference level in feature_names\n",
    "ref_index = [np.where(feature_names == \"VehBrand_B12\")[0][0],\n",
    "             np.where(feature_names == \"Region_Centre\")[0][0]]\n",
    "ref_index.append(0) # Exposure\n",
    "\n",
    "# remove reference levels\n",
    "train_dummy = np.delete(train, ref_index, axis = 1)\n",
    "test_dummy = np.delete(test, ref_index, axis = 1)\n",
    "feature_dummy = [feature for i, feature in enumerate(feature_names) if i not in ref_index]\n",
    "\n",
    "# add a constant to the model (intercept)\n",
    "train_dummy = sm.add_constant(train_dummy)\n",
    "test_dummy = sm.add_constant(test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- fit a GLM model --------------------------\n",
    "model_GLM = sm.GLM(y_train, train_dummy, family = sm.families.Poisson(), offset = offset_train)\n",
    "results = model_GLM.fit()\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056988671710803546\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluation --------------------------\n",
    "GLM_pred = results.predict(test_dummy, offset = offset_test)\n",
    "mse_GLM = mse(y_test, GLM_pred)\n",
    "print(mse_GLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 4. NEURAL NETWORKS ############################\n",
    "\n",
    "#-------------------------- preprocessing --------------------------\n",
    "# separate exposure with other features\n",
    "train_exposure = train[:,0]\n",
    "test_exposure = test[:,0]\n",
    "train_NN = train[:,1:]\n",
    "test_NN = test[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "12713/12713 [==============================] - 8s 632us/step - loss: 0.2088 - mean_squared_error: 0.0570 - poisson: 0.2088 - val_loss: 0.2079 - val_mean_squared_error: 0.0585 - val_poisson: 0.2079\n",
      "Epoch 2/50\n",
      "12713/12713 [==============================] - 8s 630us/step - loss: 0.2055 - mean_squared_error: 0.0565 - poisson: 0.2055 - val_loss: 0.2060 - val_mean_squared_error: 0.0586 - val_poisson: 0.2060\n",
      "Epoch 3/50\n",
      "12713/12713 [==============================] - 8s 608us/step - loss: 0.2044 - mean_squared_error: 0.0565 - poisson: 0.2044 - val_loss: 0.2060 - val_mean_squared_error: 0.0587 - val_poisson: 0.2060\n",
      "Epoch 4/50\n",
      "12713/12713 [==============================] - 8s 610us/step - loss: 0.2038 - mean_squared_error: 0.0565 - poisson: 0.2038 - val_loss: 0.2055 - val_mean_squared_error: 0.0587 - val_poisson: 0.2055\n",
      "Epoch 5/50\n",
      "12713/12713 [==============================] - 8s 616us/step - loss: 0.2034 - mean_squared_error: 0.0564 - poisson: 0.2034 - val_loss: 0.2051 - val_mean_squared_error: 0.0583 - val_poisson: 0.2051\n",
      "Epoch 6/50\n",
      "12713/12713 [==============================] - 8s 611us/step - loss: 0.2031 - mean_squared_error: 0.0564 - poisson: 0.2031 - val_loss: 0.2054 - val_mean_squared_error: 0.0583 - val_poisson: 0.2054\n",
      "Epoch 7/50\n",
      "12713/12713 [==============================] - 8s 607us/step - loss: 0.2029 - mean_squared_error: 0.0564 - poisson: 0.2029 - val_loss: 0.2051 - val_mean_squared_error: 0.0584 - val_poisson: 0.2051\n",
      "Epoch 8/50\n",
      "12713/12713 [==============================] - 9s 671us/step - loss: 0.2027 - mean_squared_error: 0.0563 - poisson: 0.2027 - val_loss: 0.2054 - val_mean_squared_error: 0.0592 - val_poisson: 0.2054\n",
      "Epoch 9/50\n",
      "12713/12713 [==============================] - 8s 617us/step - loss: 0.2025 - mean_squared_error: 0.0563 - poisson: 0.2025 - val_loss: 0.2049 - val_mean_squared_error: 0.0587 - val_poisson: 0.2049\n",
      "Epoch 10/50\n",
      "12713/12713 [==============================] - 8s 608us/step - loss: 0.2023 - mean_squared_error: 0.0563 - poisson: 0.2023 - val_loss: 0.2051 - val_mean_squared_error: 0.0584 - val_poisson: 0.2051\n",
      "Epoch 11/50\n",
      "12713/12713 [==============================] - 8s 614us/step - loss: 0.2020 - mean_squared_error: 0.0564 - poisson: 0.2020 - val_loss: 0.2049 - val_mean_squared_error: 0.0585 - val_poisson: 0.2049\n",
      "Epoch 12/50\n",
      "12713/12713 [==============================] - 8s 609us/step - loss: 0.2018 - mean_squared_error: 0.0563 - poisson: 0.2018 - val_loss: 0.2048 - val_mean_squared_error: 0.0587 - val_poisson: 0.2048\n",
      "Epoch 13/50\n",
      "12713/12713 [==============================] - 8s 608us/step - loss: 0.2017 - mean_squared_error: 0.0563 - poisson: 0.2017 - val_loss: 0.2049 - val_mean_squared_error: 0.0583 - val_poisson: 0.2049\n",
      "Epoch 14/50\n",
      "12713/12713 [==============================] - 8s 608us/step - loss: 0.2016 - mean_squared_error: 0.0562 - poisson: 0.2016 - val_loss: 0.2050 - val_mean_squared_error: 0.0592 - val_poisson: 0.2050\n",
      "Epoch 15/50\n",
      "12713/12713 [==============================] - 8s 616us/step - loss: 0.2014 - mean_squared_error: 0.0563 - poisson: 0.2014 - val_loss: 0.2048 - val_mean_squared_error: 0.0586 - val_poisson: 0.2048\n",
      "Epoch 16/50\n",
      "12713/12713 [==============================] - 8s 631us/step - loss: 0.2013 - mean_squared_error: 0.0561 - poisson: 0.2013 - val_loss: 0.2052 - val_mean_squared_error: 0.0584 - val_poisson: 0.2052\n",
      "Epoch 17/50\n",
      "12713/12713 [==============================] - 8s 633us/step - loss: 0.2010 - mean_squared_error: 0.0562 - poisson: 0.2010 - val_loss: 0.2051 - val_mean_squared_error: 0.0590 - val_poisson: 0.2051\n",
      "Epoch 18/50\n",
      "12713/12713 [==============================] - 8s 613us/step - loss: 0.2010 - mean_squared_error: 0.0563 - poisson: 0.2010 - val_loss: 0.2049 - val_mean_squared_error: 0.0593 - val_poisson: 0.2049\n",
      "Epoch 19/50\n",
      "12713/12713 [==============================] - 8s 610us/step - loss: 0.2008 - mean_squared_error: 0.0561 - poisson: 0.2008 - val_loss: 0.2052 - val_mean_squared_error: 0.0583 - val_poisson: 0.2052\n",
      "Epoch 20/50\n",
      "12713/12713 [==============================] - 8s 616us/step - loss: 0.2007 - mean_squared_error: 0.0561 - poisson: 0.2007 - val_loss: 0.2053 - val_mean_squared_error: 0.0584 - val_poisson: 0.2053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x31b6a5050>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------- fit a deep NN --------------------------\n",
    "\n",
    "'''Define model architecture'''\n",
    "# input layers\n",
    "exposure = Input(shape=(1,))\n",
    "other_inputs = Input(shape = train_NN.shape[1:])\n",
    "\n",
    "# dense layers\n",
    "x = Dense(64, \"relu\")(other_inputs)\n",
    "x = Dense(64, \"relu\")(x)\n",
    "lambda_ = Dense(1, \"exponential\")(x)\n",
    "\n",
    "# final output\n",
    "out = Multiply()([lambda_, exposure])\n",
    "model_NN = Model([exposure, other_inputs], out)\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 5, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# compile the model\n",
    "model_NN.compile(optimizer = \"adam\",\n",
    "              loss = \"poisson\", \n",
    "              metrics = ['mean_squared_error', 'poisson'])\n",
    "\n",
    "# fit\n",
    "model_NN.fit([train_exposure, train_NN], y_train,\n",
    "            epochs = 50,  \n",
    "            batch_size = 32, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 2s 350us/step - loss: 0.2042 - mean_squared_error: 0.0571 - poisson: 0.2042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.20424488186836243, 0.05705279856920242, 0.20424488186836243]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------------------------- evaluate on test set --------------------------\n",
    "model_NN.evaluate([test_exposure, test_NN], y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
