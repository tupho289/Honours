{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 0. PREPARATION ############################\n",
    "\n",
    "#-------------------------- import packages --------------------------\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from itertools import combinations\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input, Multiply, Add, Embedding, Reshape, Concatenate, Dropout, BatchNormalization, Lambda\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Zeros\n",
    "from tensorflow.keras.optimizers.legacy import Adam, Nadam # the non-legacy version runs slowly on Mac\n",
    "import keras_tuner as kt\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.losses import Poisson\n",
    "from scipy.stats import gamma\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.compose import make_column_transformer\n",
    "from NAM_models import ActivationLayer, FeatureNN, NAM\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "#-------------------------- import data --------------------------\n",
    "'''we use the popular French Motor TPL insurance claim data '''\n",
    "freq = pd.read_csv(\"data/freMTPL2freq.csv\")\n",
    "sev = pd.read_csv(\"data/freMTPL2sev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 1. PREPROCESSING ############################\n",
    "\n",
    "#-------------------------- merge/filter claim data --------------------------\n",
    "# remove outliers\n",
    "sev_sum = sev.groupby('IDpol').agg(\n",
    "    ClaimNb_sev = ('IDpol', 'count'),\n",
    "    Avg_Clamt = ('ClaimAmount', 'mean')\n",
    ")\n",
    "sev_sum = sev_sum[np.array(sev_sum[\"ClaimNb_sev\"] <= 4)] # remove outliers and policies that do not appear in freq data (ClaimNb > 16 do not appear in freq data)\n",
    "freq_sum = freq[np.array(freq[\"ClaimNb\"] <= 4)] # remove outliers\n",
    "\n",
    "# combine claim frequency and severity\n",
    "claim = freq_sum.merge(sev_sum, on = 'IDpol', how = 'left')\n",
    "claim['Avg_Clamt'] = claim['Avg_Clamt'].fillna(0)\n",
    "claim_final = claim.drop([\"IDpol\", \"ClaimNb_sev\"], axis = 1)\n",
    "\n",
    "\n",
    "#-------------------------- subsample and split --------------------------\n",
    "# train-test-split\n",
    "claim_final['ClaimNb_Cat'] = claim_final[\"ClaimNb\"].astype(str)\n",
    "X_train, X_test, y_train_mult, y_test_mult = train_test_split(\n",
    "    claim_final.drop([\"ClaimNb\", \"ClaimNb_Cat\", \"Avg_Clamt\"], axis = 1), \n",
    "    claim_final[[\"ClaimNb\", \"Avg_Clamt\"]], \n",
    "    stratify = claim_final[\"ClaimNb_Cat\"], # stratify sampling to ensure equal distribution of non-zero claims across train and test sets\n",
    "    random_state = 2025)\n",
    "\n",
    "y_train = y_train_mult[[\"ClaimNb\"]].squeeze() # seperate the target for single task and multitask learning\n",
    "y_test = y_test_mult[[\"ClaimNb\"]].squeeze() # seperate the target for single task and multitask learning\n",
    "claim_final = claim_final.drop(\"ClaimNb_Cat\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10054878097157874\n",
      "0.1002555031162374\n"
     ]
    }
   ],
   "source": [
    "'''check if claim count is balanced between train and test sets'''\n",
    "print(sum(y_train)/sum(X_train[\"Exposure\"]))\n",
    "print(sum(y_test)/sum(X_test[\"Exposure\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Exposure' 'Area' 'VehGas' 'VehBrand' 'Region' 'VehPower' 'VehAge'\n",
      " 'DrivAge' 'BonusMalus' 'Density']\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- feature transformation --------------------------\n",
    "\n",
    "# define transformer\n",
    "ct = make_column_transformer(\n",
    "    (\"passthrough\", [\"Exposure\"]),\n",
    "    (OrdinalEncoder(), [\"Area\", \"VehGas\", \"VehBrand\", \"Region\"]),\n",
    "    remainder = StandardScaler(),\n",
    "    verbose_feature_names_out = False\n",
    ")\n",
    "\n",
    "\n",
    "# transform the data for NAM\n",
    "train_NAM = ct.fit_transform(X_train)\n",
    "test_NAM = ct.transform(X_test)\n",
    "feature_names = ct.get_feature_names_out()  # get the columns' names\n",
    "print(feature_names)\n",
    "\n",
    "# # feature summary\n",
    "# feature_expansion = {} # number of columns for each feature after transformation\n",
    "# for original_feature in X_train.columns:\n",
    "#     # For each original feature, count how many transformed feature names start with it\n",
    "#     count = sum(fn.startswith(original_feature) for fn in feature_names)\n",
    "#     feature_expansion[original_feature] = count\n",
    "# print(feature_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 2. NAM ############################\n",
    "\n",
    "categorical_var = [\"Area\", \"VehBrand\", \"Region\"]\n",
    "\n",
    "#-------------------------- define functions to create NAM and subnets --------------------------\n",
    "'''create subnetwork for each feature/group of features'''\n",
    "def create_subnet(num_layers, units_per_layer, activation, dropout_rate):\n",
    "    \"\"\"create a subnet with configurable layers and neurons.\"\"\"\n",
    "    model = Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(units_per_layer, activation = activation))\n",
    "        model.add(Dropout(dropout_rate, seed = 2000))\n",
    "    model.add(Dense(1, activation = 'tanh'))\n",
    "    return model\n",
    "\n",
    "\n",
    "'''create NAM'''\n",
    "def build_NAM(hp):\n",
    "    inputs = []  # Store input layers\n",
    "    interaction_inputs = [] # input for modeling pairwise interaction effect\n",
    "    sub_outputs = []  # Outputs from each subnet\n",
    "\n",
    "    # hyperparameters shared across all subnets\n",
    "    num_layers = hp.Int('num_layers', 2, 4)\n",
    "    units_main = hp.Int('units_main', 10, 25, step = 5) # number of units per layer for main effect\n",
    "    units_interaction = hp.Int('units_interaction', 15, 30, step = 5) # number of units per layer for interaction effect\n",
    "    activation = hp.Choice('activation', ['relu', 'leaky_relu', 'swish', 'gelu'])\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.2, 0.5, step = 0.1)\n",
    "    embedding_dim = hp.Int(\"embedding_dimension\", 2, 4) # embedding dimension for categorical variables\n",
    "    lr = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log') # learning rate for optimizer\n",
    "\n",
    "    # main effect\n",
    "    for name in feature_names:\n",
    "        input_layer = Input(shape = (1,), name = name)\n",
    "        inputs.append(input_layer)\n",
    "        \n",
    "        if name == \"Exposure\":  # Direct use without a subnet\n",
    "            exposure_input = input_layer\n",
    "        elif name in categorical_var:\n",
    "            # categorical variables will pass through an embedding layer\n",
    "            embed_layer = Embedding(input_dim = claim_final[name].nunique(), \n",
    "                        output_dim = embedding_dim, \n",
    "                        name = f\"embed_{name}\")(input_layer)\n",
    "            embed_layer_reshape = Reshape(target_shape = (embedding_dim,), name = f\"reshape_{name}\")(embed_layer)\n",
    "            interaction_inputs.append(embed_layer_reshape)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(embed_layer_reshape)\n",
    "            sub_outputs.append(sub_output)\n",
    "        else:\n",
    "            interaction_inputs.append(input_layer)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(input_layer)\n",
    "            sub_outputs.append(sub_output)\n",
    "    \n",
    "    # interaction effect\n",
    "    for input_1, input_2 in combinations(interaction_inputs, 2): # we don't include Exposure\n",
    "        # for categorical input\n",
    "        interaction_input = Concatenate()([input_1, input_2])\n",
    "        interaction_subnet = create_subnet(num_layers, units_interaction, activation, dropout_rate)\n",
    "        interaction_output = interaction_subnet(interaction_input)\n",
    "        sub_outputs.append(interaction_output)\n",
    "\n",
    "    # combine subnets' outputs\n",
    "    subnet_output = Concatenate()(sub_outputs)\n",
    "    output_layer = Dense(1, activation = \"exponential\")(subnet_output)\n",
    "\n",
    "    # multiply output with exposure\n",
    "    final_output = Multiply()([exposure_input, output_layer])\n",
    "    model = Model(inputs = inputs, outputs = final_output)\n",
    "    \n",
    "    #compile and return model\n",
    "    model.compile(optimizer = Nadam(learning_rate = lr), loss='poisson', metrics=['mean_squared_error', 'poisson'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hyperparameter_tuning_NAM\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in hyperparameter_tuning_NAM\\untitled_project\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000264938FAF70>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_main: 20\n",
      "units_interaction: 25\n",
      "activation: swish\n",
      "dropout_rate: 0.4\n",
      "embedding_dimension: 4\n",
      "learning_rate: 0.0010799944024561447\n",
      "Score: 0.2021609991788864\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- fit a NAM to training data --------------------------\n",
    "\n",
    "# hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_NAM,\n",
    "    objective = 'val_loss',\n",
    "    max_trials = 5,  # Increased trials due to additional hyperparameters\n",
    "    directory = \"hyperparameter_tuning_NAM\",\n",
    "    seed = 2025 # for reproducibility\n",
    ")\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 10, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# training data need to be split into different arrays, each correponds to input for a particular subnet\n",
    "X_train_split = []\n",
    "for i in range(len(feature_names)):\n",
    "    X_train_split.append(train_NAM[:, i])\n",
    "\n",
    "# search for the best model\n",
    "tuner.search(X_train_split, y_train,\n",
    "            epochs = 100,  \n",
    "            batch_size = 1000, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])\n",
    "\n",
    "# get the best model\n",
    "model_NAM = tuner.get_best_models()[0]\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 16s 3ms/step - loss: 0.2019 - mean_squared_error: 0.0552 - poisson: 0.2019\n",
      "NAM test set metrics ~ Poisson loss: 0.2019, MSE: 0.0552\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on testing data --------------------------\n",
    "\n",
    "# split testing data into different sets\n",
    "X_test_split = []\n",
    "for i in range(len(feature_names)):\n",
    "    X_test_split.append(test_NAM[:, i])\n",
    "\n",
    "# evaluation\n",
    "nam_poisson_loss, nam_mse, _ = model_NAM.evaluate(X_test_split, y_test)\n",
    "print(f\"NAM test set metrics ~ Poisson loss: {nam_poisson_loss:.4f}, MSE: {nam_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 3. NAM - multitask ############################\n",
    "\n",
    "'''loss function for multitask learning - assume Gamma distribution for claim amount'''\n",
    "def multitask_loss(y_true, y_pred):\n",
    "    claim_freq = y_pred[:,0]\n",
    "    gamma_scale = y_pred[:,1]\n",
    "    gamma_shape = y_pred[:,2]\n",
    "\n",
    "    poisson_loss = Poisson()(y_true[:,0], claim_freq)\n",
    "    \n",
    "    # Calculating Gamma loss\n",
    "    epsilon = 1e-7  # to avoid log(0)\n",
    "    gamma_loss = -(\n",
    "        (gamma_shape - 1) * tf.math.log(y_true[:, 1]+ epsilon) \n",
    "        - y_true[:, 1] / gamma_scale \n",
    "        - gamma_shape * tf.math.log(gamma_scale + epsilon)\n",
    "        - tf.math.lgamma(gamma_shape + epsilon)\n",
    "    )\n",
    "    \n",
    "    sev_weights = tf.where(y_true[:,0] > 0, 0.5, 0.0) # weighting factor for claim severity\n",
    "\n",
    "    # Weighted average loss\n",
    "    total_loss = (1 - sev_weights) * poisson_loss + sev_weights * gamma_loss\n",
    "    return tf.reduce_mean(total_loss)\n",
    "\n",
    "'''calculate the metric of claim frequency'''\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return tf.keras.metrics.mean_squared_error(y_true[:, 0], y_pred[:, 0])\n",
    "\n",
    "def custom_poisson(y_true, y_pred):\n",
    "    return Poisson()(y_true[:,0], y_pred[:,0])\n",
    "\n",
    "'''create multi-task model'''\n",
    "def build_multitask_NAM(hp):\n",
    "    inputs = []  # Store input layers\n",
    "    interaction_inputs = [] # input for modeling pairwise interaction effect\n",
    "    sub_outputs = []  # Outputs from each subnet\n",
    "\n",
    "    # hyperparameters shared across all subnets\n",
    "    num_layers = hp.Int('num_layers', 2, 4)\n",
    "    units_main = hp.Int('units_main', 10, 25, step = 5) # number of units per layer for main effect\n",
    "    units_interaction = hp.Int('units_interaction', 15, 30, step = 5) # number of units per layer for interaction effect\n",
    "    activation = hp.Choice('activation', ['relu', 'leaky_relu', 'swish', 'gelu'])\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.2, 0.5, step = 0.1)\n",
    "    embedding_dim = hp.Int(\"embedding_dimension\", 2, 4) # embedding dimension for categorical variables\n",
    "    lr = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log') # learning rate for optimizer\n",
    "\n",
    "    # main effect\n",
    "    for name in feature_names:\n",
    "        input_layer = Input(shape = (1,), name = name)\n",
    "        inputs.append(input_layer)\n",
    "        \n",
    "        if name == \"Exposure\":  # Direct use without a subnet\n",
    "            exposure_input = input_layer\n",
    "        elif name in categorical_var:\n",
    "            # categorical variables will pass through an embedding layer\n",
    "            embed_layer = Embedding(input_dim = claim_final[name].nunique(), \n",
    "                        output_dim = embedding_dim, \n",
    "                        name = f\"embed_{name}\")(input_layer)\n",
    "            embed_layer_reshape = Reshape(target_shape = (embedding_dim,), name = f\"reshape_{name}\")(embed_layer)\n",
    "            interaction_inputs.append(embed_layer_reshape)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(embed_layer_reshape)\n",
    "            sub_outputs.append(sub_output)\n",
    "        else:\n",
    "            interaction_inputs.append(input_layer)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(input_layer)\n",
    "            sub_outputs.append(sub_output)\n",
    "    \n",
    "    # interaction effect\n",
    "    for input_1, input_2 in combinations(interaction_inputs, 2): # we don't include Exposure\n",
    "        # for categorical input\n",
    "        interaction_input = Concatenate()([input_1, input_2])\n",
    "        interaction_subnet = create_subnet(num_layers, units_interaction, activation, dropout_rate)\n",
    "        interaction_output = interaction_subnet(interaction_input)\n",
    "        sub_outputs.append(interaction_output)\n",
    "\n",
    "    # combine subnets' outputs\n",
    "    subnet_output = Concatenate()(sub_outputs)\n",
    "    \n",
    "    # Define separate output neurons for claim frequency, gamma scale, and gamma shape\n",
    "    claim_freq_output = Dense(1, activation = \"exponential\", name ='claim_freq')(subnet_output)\n",
    "    gamma_scale_output = Dense(1, activation = \"exponential\", name = 'gamma_scale')(subnet_output)\n",
    "    gamma_shape_output = Dense(1, activation = \"exponential\", name ='gamma_shape')(subnet_output)\n",
    "\n",
    "    # multiply claim frequency with exposure\n",
    "    claim_freq_adjusted = Multiply()([exposure_input, claim_freq_output])\n",
    "\n",
    "    # concatenate adjusted claim frequency with gamma parameters\n",
    "    final_output = Concatenate()([claim_freq_adjusted, gamma_scale_output, gamma_shape_output])\n",
    "\n",
    "    model = Model(inputs = inputs, outputs = final_output)\n",
    "    \n",
    "    #compile and return model\n",
    "    model.compile(optimizer = Nadam(learning_rate = lr), \n",
    "                  loss = multitask_loss, \n",
    "                   metrics=[custom_mse, custom_poisson])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hyperparameter_tuning_multitask_NAM\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in hyperparameter_tuning_multitask_NAM\\untitled_project\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x00000264938FAB50>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_main: 20\n",
      "units_interaction: 20\n",
      "activation: swish\n",
      "dropout_rate: 0.30000000000000004\n",
      "embedding_dimension: 2\n",
      "learning_rate: 0.007836255783976532\n",
      "Score: 0.29096662998199463\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- tuning and fit the model to training data --------------------------\n",
    "# hyperparameter tuning\n",
    "tuner = kt.RandomSearch(\n",
    "    build_multitask_NAM,\n",
    "    objective = 'val_loss',\n",
    "    max_trials = 5,  # Increased trials due to additional hyperparameters\n",
    "    directory = \"hyperparameter_tuning_multitask_NAM\",\n",
    "    seed = 2025 # for reproducibility\n",
    ")\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 3, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# search for the best model\n",
    "tuner.search(X_train_split, y_train_mult,\n",
    "            epochs = 100,  \n",
    "            batch_size = 1000, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])\n",
    "\n",
    "# get the best model\n",
    "model_multitask_NAM = tuner.get_best_models()[0]\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 14s 2ms/step - loss: 0.2902 - custom_mse: 0.0555 - custom_poisson: 0.2035\n",
      "NAM-multitask test set metrics ~ Poisson loss: 0.2035, MSE: 0.0555\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on testing data --------------------------\n",
    "_, nam_multitask_mse, nam_multitask_poisson_loss, = model_multitask_NAM.evaluate(X_test_split, y_test_mult)\n",
    "print(f\"NAM-multitask test set metrics ~ Poisson loss: {nam_multitask_poisson_loss:.4f}, MSE: {nam_multitask_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 4. GLM ############################\n",
    "\n",
    "#-------------------------- preprocessing --------------------------\n",
    "\n",
    "# define transformer\n",
    "ct = make_column_transformer(\n",
    "    (\"passthrough\", [\"Exposure\"]),\n",
    "    (OrdinalEncoder(), [\"Area\", \"VehGas\"]),\n",
    "    (OneHotEncoder(), [\"VehBrand\", \"Region\"]),\n",
    "    remainder = StandardScaler(),\n",
    "    verbose_feature_names_out = False\n",
    ")\n",
    "\n",
    "# fit & transform\n",
    "train = ct.fit_transform(X_train).toarray()\n",
    "test = ct.transform(X_test).toarray()\n",
    "feature_names = ct.get_feature_names_out()  # get the columns' names\n",
    "\n",
    "\n",
    "'''we want to use dummy encoding for VehBrand and Region so 2 columns need\n",
    "to be removed from both training and test data. These 2 columns are the reference\n",
    "levels for VehBrand and Region. Choose B12 for VehBrand and Centre for Region.'''\n",
    "\n",
    "# separate the offset term (or exposure)\n",
    "offset_train = train[:, 0]\n",
    "offset_test = test[:, 0]\n",
    "\n",
    "# index for reference level in feature_names\n",
    "ref_index = [np.where(feature_names == \"VehBrand_B12\")[0][0],\n",
    "             np.where(feature_names == \"Region_Centre\")[0][0]]\n",
    "ref_index.append(0) # Exposure\n",
    "\n",
    "# remove reference levels\n",
    "train_dummy = np.delete(train, ref_index, axis = 1)\n",
    "test_dummy = np.delete(test, ref_index, axis = 1)\n",
    "feature_dummy = [feature for i, feature in enumerate(feature_names) if i not in ref_index]\n",
    "\n",
    "# add a constant to the model (intercept)\n",
    "train_dummy = sm.add_constant(train_dummy)\n",
    "test_dummy = sm.add_constant(test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  4.        ,  1.        , ...,  0.24853382,\n",
       "        -0.62435238,  0.15282861],\n",
       "       [ 1.        ,  1.        ,  0.        , ..., -0.24667445,\n",
       "        -0.62435238, -0.43372675],\n",
       "       [ 1.        ,  4.        ,  0.        , ...,  0.53150997,\n",
       "        -0.3689211 ,  0.07786865],\n",
       "       ...,\n",
       "       [ 1.        ,  3.        ,  1.        , ..., -1.59081117,\n",
       "         2.24924957, -0.04857896],\n",
       "       [ 1.        ,  1.        ,  0.        , ...,  0.74374208,\n",
       "         0.78051968, -0.43675543],\n",
       "       [ 1.        ,  3.        ,  1.        , ...,  0.46076593,\n",
       "        -0.62435238, -0.26588711]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- fit a GLM model --------------------------\n",
    "model_GLM = sm.GLM(y_train, train_dummy, family = sm.families.Poisson(), offset = offset_train)\n",
    "results = model_GLM.fit()\n",
    "# results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  4.        ,  1.        , ...,  0.24853382,\n",
       "        -0.62435238,  0.15282861],\n",
       "       [ 1.        ,  1.        ,  0.        , ..., -0.24667445,\n",
       "        -0.62435238, -0.43372675],\n",
       "       [ 1.        ,  4.        ,  0.        , ...,  0.53150997,\n",
       "        -0.3689211 ,  0.07786865],\n",
       "       ...,\n",
       "       [ 1.        ,  3.        ,  1.        , ..., -1.59081117,\n",
       "         2.24924957, -0.04857896],\n",
       "       [ 1.        ,  1.        ,  0.        , ...,  0.74374208,\n",
       "         0.78051968, -0.43675543],\n",
       "       [ 1.        ,  3.        ,  1.        , ...,  0.46076593,\n",
       "        -0.62435238, -0.26588711]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05560565395994129\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluation --------------------------\n",
    "GLM_pred = results.predict(test_dummy, offset = offset_test)\n",
    "GLM_mse = mse(y_test, GLM_pred)\n",
    "print(GLM_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20274714124657348"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the Poisson loss for the GLM model which Keras uses\n",
    "glm_poisson_loss = keras.losses.Poisson()(y_test, GLM_pred).numpy()\n",
    "glm_poisson_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 5. NEURAL NETWORKS ############################\n",
    "\n",
    "#-------------------------- preprocessing --------------------------\n",
    "ct_NN = make_column_transformer(\n",
    "    (\"passthrough\", [\"Exposure\"]),\n",
    "    (OrdinalEncoder(), [\"Area\", \"VehGas\", \"VehBrand\", \"Region\"]),\n",
    "    remainder = StandardScaler(),\n",
    "    verbose_feature_names_out = False)\n",
    "\n",
    "\n",
    "# transform the data\n",
    "train_NN = ct_NN.fit_transform(X_train)\n",
    "test_NN = ct_NN.transform(X_test)\n",
    "feature_names_NN = ct_NN.get_feature_names_out()  # get the columns' names\n",
    "\n",
    "\n",
    "# separate exposure column\n",
    "exposure_index = np.where(feature_names_NN == \"Exposure\")[0][0] # index of the Exposure column\n",
    "train_exposure = train_NN[:,exposure_index]\n",
    "test_exposure = test_NN[:,exposure_index]\n",
    "\n",
    "\n",
    "# separate multi-level categorical columns\n",
    "categorical_var = [\"Area\", \"VehBrand\", \"Region\"]\n",
    "train_cat = []\n",
    "test_cat = []\n",
    "cat_index = []\n",
    "for i in range(len(categorical_var)):\n",
    "    index = np.where(feature_names_NN == categorical_var[i])[0][0]\n",
    "    train_cat.append(train_NN[:,index])\n",
    "    test_cat.append(test_NN[:,index])\n",
    "    cat_index.append(index)\n",
    "\n",
    "                    \n",
    "# drop exposure and nominal categorical columns\n",
    "train_others = np.delete(train_NN, [exposure_index] + cat_index, axis = 1)\n",
    "test_others = np.delete(test_NN, [exposure_index] + cat_index, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- define model architecture --------------------------\n",
    "\n",
    "'''Define model architecture'''\n",
    "def build_NN(hp):\n",
    "    \n",
    "    exposure = Input(shape=(1,), name = \"exposure\") # exposure\n",
    "    cat_inputs = [] # input layers for categorical features\n",
    "    embedding_layers = [] # embedding layers\n",
    "    embedding_dim = hp.Int('embedding_dim', 2, 4, step = 1)\n",
    "    for cat_column in categorical_var:\n",
    "\n",
    "        # input layers\n",
    "        cat_input = Input(shape = (1,), name = f\"input_{cat_column}\")  # assuming categorical vars are single integers\n",
    "        cat_inputs.append(cat_input)\n",
    "\n",
    "        # embedding layers\n",
    "        embed_layer = Embedding(input_dim = claim_final[cat_column].nunique(), \n",
    "                                output_dim = embedding_dim, \n",
    "                                name = f\"embed_{cat_column}\")(cat_input)\n",
    "        embed_layer_reshape = Reshape(target_shape = (embedding_dim,), name = f\"reshape_{cat_column}\")(embed_layer)\n",
    "\n",
    "        embedding_layers.append(embed_layer_reshape)\n",
    "        \n",
    "    other_inputs = Input(shape = train_others.shape[1:])\n",
    "    inputs = Concatenate(name = \"combined_input\")(embedding_layers + [other_inputs])\n",
    "\n",
    "    # dense layers\n",
    "    x = inputs\n",
    "    activation = hp.Choice('activation', ['relu', 'leaky_relu', 'swish', 'gelu'])\n",
    "    dropout_rate = hp.Float(f'dropout', min_value = 0.2, max_value = 0.5, step = 0.1)\n",
    "    for i in range(hp.Int('num_dense_layers', 2, 5, step = 2)):\n",
    "        num_units = hp.Int(f'num_units_{i}', 32, 128, step = 16)\n",
    "        x = Dense(units = num_units, activation = activation)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    lambda_ = Dense(1, \"exponential\")(x)\n",
    "\n",
    "    # final output\n",
    "    out = Multiply()([lambda_, exposure])\n",
    "    model = Model([exposure] + cat_inputs + [other_inputs], out)\n",
    "\n",
    "    # optimizer\n",
    "    lr = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log')\n",
    "    optimizer = Nadam(learning_rate = lr)\n",
    "    \n",
    "    # build model\n",
    "    model.compile(optimizer = optimizer,\n",
    "              loss = \"poisson\", \n",
    "              metrics = ['mean_squared_error', 'poisson'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hyperparameter_tuning_NN\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in hyperparameter_tuning_NN\\untitled_project\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000026496725C70>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "embedding_dim: 3\n",
      "activation: relu\n",
      "dropout: 0.2\n",
      "num_dense_layers: 2\n",
      "num_units_0: 112\n",
      "num_units_1: 64\n",
      "learning_rate: 0.0021291840426046296\n",
      "Score: 0.2021438479423523\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- tune and fit fit a deep NN --------------------------\n",
    "# tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_NN,\n",
    "    objective = 'val_loss',\n",
    "    max_trials = 5,  # Increased trials due to additional hyperparameters\n",
    "    directory = \"hyperparameter_tuning_NN\",\n",
    "    seed = 2025 # for reproducibility\n",
    ")\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 10, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# search for the best model\n",
    "tuner.search([train_exposure, train_cat, train_others], y_train,\n",
    "            epochs = 500,  \n",
    "            batch_size = 1000, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])\n",
    "\n",
    "# get the best model\n",
    "model_NN = tuner.get_best_models()[0]\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- plot model --------------------------\n",
    "plot_model(model_NN, show_layer_names = True, show_shapes = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 4s 720us/step - loss: 0.2017 - mean_squared_error: 0.0551 - poisson: 0.2017\n",
      "NN test set metrics ~ Poisson loss: 0.2017, MSE: 0.0551\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on test set --------------------------\n",
    "nn_poisson_loss, nn_mse, _ = model_NN.evaluate([test_exposure, test_cat, test_others], y_test)\n",
    "print(f\"NN test set metrics ~ Poisson loss: {nn_poisson_loss:.4f}, MSE: {nn_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 6. NAM - sparse ############################\n",
    "\n",
    "#-------------------------- define functions to create NAM and subnets --------------------------\n",
    "'''create subnetwork for each feature/group of features'''\n",
    "def create_subnet(num_layers, units_per_layer, activation, dropout_rate):\n",
    "    \"\"\"create a subnet with configurable layers and neurons.\"\"\"\n",
    "    model = Sequential()\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(units_per_layer, activation = activation))\n",
    "        model.add(Dropout(dropout_rate, seed = 2000))\n",
    "    model.add(Dense(1))\n",
    "    model.add(BatchNormalization())\n",
    "    return model\n",
    "\n",
    "\n",
    "'''create NAM'''\n",
    "def build_NAM_sparse(hp):\n",
    "    inputs = []  # Store input layers\n",
    "    interaction_inputs = [] # input for modeling pairwise interaction effect\n",
    "    sub_outputs = []  # Outputs from each subnet\n",
    "\n",
    "    # hyperparameters shared across all subnets\n",
    "    num_layers = hp.Int('num_layers', 2, 4)\n",
    "    units_main = hp.Int('units_main', 10, 25, step = 5) # number of units per layer for main effect\n",
    "    units_interaction = hp.Int('units_interaction', 15, 30, step = 5) # number of units per layer for interaction effect\n",
    "    activation = hp.Choice('activation', ['relu', 'leaky_relu', 'swish', 'gelu'])\n",
    "    dropout_rate = hp.Float('dropout_rate', 0.2, 0.5, step = 0.1)\n",
    "    embedding_dim = hp.Int(\"embedding_dimension\", 2, 4) # embedding dimension for categorical variables\n",
    "    lr = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log') # learning rate for optimizer\n",
    "    lambda_val = hp.Float('lambda', min_value = 1e-7, max_value = 1e-3, sampling = 'log')\n",
    "\n",
    "    # main effect\n",
    "    for name in feature_names_NN:\n",
    "        input_layer = Input(shape = (1,), name = name)\n",
    "        inputs.append(input_layer)\n",
    "        \n",
    "        if name == \"Exposure\":  # Direct use without a subnet\n",
    "            exposure_input = input_layer\n",
    "        elif name in categorical_var:\n",
    "            # categorical variables will pass through an embedding layer\n",
    "            embed_layer = Embedding(input_dim = claim_final[name].nunique(), \n",
    "                        output_dim = embedding_dim, \n",
    "                        name = f\"embed_{name}\")(input_layer)\n",
    "            embed_layer_reshape = Reshape(target_shape = (embedding_dim,), name = f\"reshape_{name}\")(embed_layer)\n",
    "            interaction_inputs.append(embed_layer_reshape)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(embed_layer_reshape)\n",
    "            sub_outputs.append(sub_output)\n",
    "        else:\n",
    "            interaction_inputs.append(input_layer)\n",
    "            subnet = create_subnet(num_layers, units_main, activation, dropout_rate)\n",
    "            sub_output = subnet(input_layer)\n",
    "            sub_outputs.append(sub_output)\n",
    "    \n",
    "    # interaction effect\n",
    "    for input_1, input_2 in combinations(interaction_inputs, 2): # we don't include Exposure\n",
    "        # for categorical input\n",
    "        interaction_input = Concatenate()([input_1, input_2])\n",
    "        interaction_subnet = create_subnet(num_layers, units_interaction, activation, dropout_rate)\n",
    "        interaction_output = interaction_subnet(interaction_input)\n",
    "        sub_outputs.append(interaction_output)\n",
    "\n",
    "    # combine subnets' outputs\n",
    "    subnet_output = Concatenate()(sub_outputs)\n",
    "    final_dense_layer = Dense(1, activation=\"exponential\")\n",
    "    output_layer = final_dense_layer(subnet_output)\n",
    "\n",
    "    # multiply output with exposure\n",
    "    final_output = Multiply()([exposure_input, output_layer])\n",
    "    model = Model(inputs = inputs, outputs = final_output)\n",
    "    \n",
    "    def objective_function(y_true, y_pred):\n",
    "        base_loss = tf.keras.losses.poisson(y_true, y_pred)\n",
    "        regularization = lambda_val * tf.reduce_sum(tf.abs(final_dense_layer.kernel))\n",
    "        return base_loss + regularization\n",
    "\n",
    "    #compile and return model\n",
    "    model.compile(optimizer = Nadam(learning_rate = lr), loss = objective_function, metrics=['mean_squared_error', 'poisson'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from hyperparameter_tuning_NAM_sparse\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in hyperparameter_tuning_NAM_sparse\\untitled_project\n",
      "Showing 1 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000026493378580>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units_main: 10\n",
      "units_interaction: 30\n",
      "activation: gelu\n",
      "dropout_rate: 0.2\n",
      "embedding_dimension: 2\n",
      "learning_rate: 0.008079775183819825\n",
      "lambda: 3.369139883399427e-06\n",
      "Score: 0.20280131697654724\n"
     ]
    }
   ],
   "source": [
    "# -------------------------- fit a NAM sparse to training data --------------------------\n",
    "\n",
    "# hyperparameter tuning\n",
    "tuner_NAM_sparse = kt.RandomSearch(\n",
    "    build_NAM_sparse,\n",
    "    objective = 'val_loss',\n",
    "    max_trials = 5,  # Increased trials due to additional hyperparameters\n",
    "    directory = \"hyperparameter_tuning_NAM_sparse\",\n",
    "    seed = 2025 # for reproducibility\n",
    ")\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 10, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "# training data need to be split into different arrays, each correponds to input for a particular subnet\n",
    "X_train_split = []\n",
    "for i in range(len(feature_names_NN)):\n",
    "    X_train_split.append(train_NAM[:, i])\n",
    "\n",
    "# search for the best model\n",
    "tuner_NAM_sparse.search(X_train_split, y_train,\n",
    "            epochs = 100,  \n",
    "            batch_size = 2000, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])\n",
    "\n",
    "# get the best model\n",
    "NAM_sparse = tuner_NAM_sparse.get_best_models()[0]\n",
    "tuner_NAM_sparse.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 19s 3ms/step - loss: 0.2027 - mean_squared_error: 0.0552 - poisson: 0.2027\n",
      "NAM test set metrics ~ Poisson loss: 0.2027, MSE: 0.0552\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on testing data --------------------------\n",
    "# split testing data into different sets\n",
    "X_test_split = []\n",
    "for i in range(len(feature_names_NN)):\n",
    "    X_test_split.append(test_NAM[:, i])\n",
    "\n",
    "# evaluation\n",
    "_, nam_sparse_mse, nam_sparse_poisson_loss = NAM_sparse.evaluate(X_test_split, y_test)\n",
    "print(f\"NAM test set metrics ~ Poisson loss: {nam_sparse_poisson_loss:.4f}, MSE: {nam_sparse_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06982408],\n",
       "       [ 0.02478735],\n",
       "       [ 0.40121156],\n",
       "       [-0.00322605],\n",
       "       [ 0.0071771 ],\n",
       "       [-0.03222786],\n",
       "       [-0.04049305],\n",
       "       [-0.11454726],\n",
       "       [-0.00348934],\n",
       "       [ 0.05488854],\n",
       "       [-1.0184373 ],\n",
       "       [-0.02394445],\n",
       "       [ 0.06990471],\n",
       "       [ 0.24423312],\n",
       "       [-0.08087277],\n",
       "       [ 0.2297778 ],\n",
       "       [-0.06123446],\n",
       "       [-0.09661167],\n",
       "       [ 0.00657442],\n",
       "       [ 0.06168557],\n",
       "       [-0.1518234 ],\n",
       "       [-0.0592466 ],\n",
       "       [-0.2413681 ],\n",
       "       [ 0.0065154 ],\n",
       "       [-0.14535318],\n",
       "       [-0.17051622],\n",
       "       [ 0.3123775 ],\n",
       "       [-0.07521696],\n",
       "       [-0.17325313],\n",
       "       [ 0.06939006],\n",
       "       [ 0.07103633],\n",
       "       [ 0.06774147],\n",
       "       [-0.10647857],\n",
       "       [-0.13080256],\n",
       "       [-0.06830701],\n",
       "       [-0.14871813],\n",
       "       [-0.07896812],\n",
       "       [ 0.04126915],\n",
       "       [ 0.04356374],\n",
       "       [-0.06790996],\n",
       "       [ 0.13342215],\n",
       "       [ 0.00928518],\n",
       "       [ 0.13726951],\n",
       "       [ 0.01560248],\n",
       "       [ 0.04981403]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAM_sparse.get_weights()[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tu.DESKTOP-C2DE5DP\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\interpret\\glassbox\\_ebm\\_ebm.py:995: UserWarning: Detected multiclass problem. Forcing interactions to 0. Multiclass interactions only have local explanations. They are not currently displayed in the global explanation visualizations. Set interactions=0 to disable this warning. If you still want multiclass interactions, this API accepts a list, and the measure_interactions function can be used to detect them.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExplainableBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExplainableBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>ExplainableBoostingClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExplainableBoostingClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################ 7. EBM ############################\n",
    "\n",
    "#-------------------------- fit the model --------------------------\n",
    "# # prepare data\n",
    "# train_EBM = train_dummy[:,1:]\n",
    "# test_EBM = test_dummy[:,1:]\n",
    "\n",
    "# fit the model\n",
    "model_EBM = ExplainableBoostingClassifier()\n",
    "model_EBM.fit(train_NN, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.05923858856289933\n",
      "Poisson Loss (TensorFlow): 0.8562985062599182\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on testing data --------------------------\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model_EBM.predict(test_NN)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "ebm_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {ebm_mse}')\n",
    "\n",
    "# Convert y_test and y_pred to tensors for TensorFlow Poisson loss\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
    "y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "# Calculate Poisson loss using TensorFlow\n",
    "poisson_loss = Poisson()\n",
    "ebm_poisson_loss = poisson_loss(y_test_tensor, y_pred_tensor).numpy()\n",
    "print(f'Poisson Loss (TensorFlow): {ebm_poisson_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ 8. CANN ############################\n",
    "\n",
    "#-------------------------- fit the model --------------------------\n",
    "'''Define model architecture'''\n",
    "def build_CANN(hp):\n",
    "    \n",
    "    exposure = Input(shape=(1,), name = \"exposure\") # exposure\n",
    "    cat_inputs = [] # input layers for categorical features\n",
    "    embedding_layers = [] # embedding layers\n",
    "    embedding_dim = hp.Int('embedding_dim', 2, 4, step = 1)\n",
    "    for cat_column in categorical_var:\n",
    "\n",
    "        # input layers\n",
    "        cat_input = Input(shape = (1,), name = f\"input_{cat_column}\")  # assuming categorical vars are single integers\n",
    "        cat_inputs.append(cat_input)\n",
    "\n",
    "        # embedding layers\n",
    "        embed_layer = Embedding(input_dim = claim_final[cat_column].nunique(), \n",
    "                                output_dim = embedding_dim, \n",
    "                                name = f\"embed_{cat_column}\")(cat_input)\n",
    "        embed_layer_reshape = Reshape(target_shape = (embedding_dim,), name = f\"reshape_{cat_column}\")(embed_layer)\n",
    "\n",
    "        embedding_layers.append(embed_layer_reshape)\n",
    "        \n",
    "    other_inputs = Input(shape = train_others.shape[1:])\n",
    "    inputs = Concatenate(name = \"combined_input\")(embedding_layers + [other_inputs])\n",
    "\n",
    "    # dense layers\n",
    "    x = inputs\n",
    "    activation = hp.Choice('activation', ['relu', 'leaky_relu', 'swish', 'gelu'])\n",
    "    dropout_rate = hp.Float(f'dropout', min_value = 0.2, max_value = 0.5, step = 0.1)\n",
    "    for i in range(hp.Int('num_dense_layers', 2, 5, step = 2)):\n",
    "        num_units = hp.Int(f'num_units_{i}', 32, 128, step = 16)\n",
    "        x = Dense(units = num_units, activation = activation,\n",
    "                  kernel_initializer = Zeros(),  \n",
    "                  bias_initializer = Zeros())(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    lambda_ = Dense(1, activation = 'exponential')(x)\n",
    "    nn_out = Multiply()([lambda_, exposure])\n",
    "\n",
    "    # glm skip connection\n",
    "    glm_input = Input(shape = (1,), name = 'glm_input', dtype=tf.float32)\n",
    "    glm_layer = Lambda(lambda x: x, name=\"glm_layer\")(glm_input)\n",
    "\n",
    "    # final output\n",
    "    final_output = Multiply()([nn_out, glm_input])\n",
    "    model = Model([exposure] + cat_inputs + [other_inputs] + [glm_input], final_output)\n",
    "\n",
    "    # optimizer\n",
    "    lr = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, sampling = 'log')\n",
    "    optimizer = Nadam(learning_rate = lr)\n",
    "    \n",
    "    # build model\n",
    "    model.compile(optimizer = optimizer,\n",
    "              loss = \"poisson\", \n",
    "              metrics = ['mean_squared_error', 'poisson'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------- tune and fit fit a CANN--------------------------\n",
    "# tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_CANN,\n",
    "    objective = 'val_loss',\n",
    "    max_trials = 10,  # Increased trials due to additional hyperparameters\n",
    "    directory = \"hyperparameter_tuning_CANN\",\n",
    "    seed = 2025 # for reproducibility\n",
    ")\n",
    "\n",
    "# regularization\n",
    "es = EarlyStopping(patience = 10, restore_best_weights = True, verbose = 0)\n",
    "\n",
    "train_glm = results.predict(train_dummy, offset = offset_train)\n",
    "\n",
    "# search for the best model\n",
    "tuner.search([train_exposure, train_cat, train_others, train_glm], y_train,\n",
    "            epochs = 500,  \n",
    "            batch_size = 1000, \n",
    "            validation_split = 0.2,\n",
    "            callbacks = [es])\n",
    "\n",
    "# get the best model\n",
    "model_CANN = tuner.get_best_models()[0]\n",
    "tuner.results_summary(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5297/5297 [==============================] - 4s 751us/step - loss: 0.2174 - mean_squared_error: 0.0564 - poisson: 0.2174\n",
      "CANN test set metrics ~ Poisson loss: 0.2174, MSE: 0.0564\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- evaluate on test set --------------------------\n",
    "test_glm = results.predict(test_dummy, offset = offset_test)\n",
    "cann_poisson_loss, cann_mse, _ = model_CANN.evaluate([test_exposure, test_cat, test_others, test_glm], y_test)\n",
    "print(f\"CANN test set metrics ~ Poisson loss: {cann_poisson_loss:.4f}, MSE: {cann_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM test set metrics \t Poisson loss: 0.2027 \t MSE: 0.0556\n",
      "NAM test set metrics \t Poisson loss: 0.2019 \t MSE: 0.0552\n",
      "NAM multitask test set metrics \t Poisson loss: 0.2035 \t MSE: 0.0555\n",
      "NAM sparse metrics \t Poisson loss: 0.2027 \t MSE: 0.0552\n",
      "NN test set metrics \t Poisson loss: 0.2017 \t MSE: 0.0551\n",
      "EBM test set metrics \t Poisson loss: 0.8563 \t MSE: 0.0592\n",
      "CANN test set metrics \t Poisson loss: 0.2174 \t MSE: 0.0564\n"
     ]
    }
   ],
   "source": [
    "# Print all the results\n",
    "print(f\"GLM test set metrics \\t Poisson loss: {glm_poisson_loss:.4f} \\t MSE: {GLM_mse:.4f}\")\n",
    "print(f\"NAM test set metrics \\t Poisson loss: {nam_poisson_loss:.4f} \\t MSE: {nam_mse:.4f}\")\n",
    "print(f\"NAM multitask test set metrics \\t Poisson loss: {nam_multitask_poisson_loss:.4f} \\t MSE: {nam_multitask_mse:.4f}\")\n",
    "print(f\"NAM sparse metrics \\t Poisson loss: {nam_sparse_poisson_loss:.4f} \\t MSE: {nam_sparse_mse:.4f}\")\n",
    "print(f\"NN test set metrics \\t Poisson loss: {nn_poisson_loss:.4f} \\t MSE: {nn_mse:.4f}\")\n",
    "print(f\"EBM test set metrics \\t Poisson loss: {ebm_poisson_loss:.4f} \\t MSE: {ebm_mse:.4f}\")\n",
    "print(f\"CANN test set metrics \\t Poisson loss: {cann_poisson_loss:.4f} \\t MSE: {cann_mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
